# Default system properties included when running spark-submit.
# This is useful for setting default environmental settings.

spark.app.name						            dbpedialinks                                        #provide! -> the app.name defines the pipeline to run
spark.master                     	            spark://chile-ubuntu:7077                           #provide! (you can use 'local[*]' to run on localhost)
spark.eventLog.enabled           	            true
spark.eventLog.dir               	            file:///home/chile/unifiedviews/events
# spark.serializer                 	            org.apache.spark.serializer.KryoSerializer
spark.driver.memory              	            4g
# spark.executor.extraJavaOptions  	            -XX:+PrintGCDetails -Dkey=value -Dnumbers="one two three"
spark.shuffle.manager              	            SORT
spark.shuffle.consolidateFiles     	            true
spark.shuffle.spill                	            true
spark.shuffle.memoryFraction       	            0.75
spark.storage.memoryFraction       	            0.45
spark.shuffle.spill.compress       	            false
spark.shuffle.compress             	            false
#spark.dynamicAllocation.enabled                true

spark.clientSparkVersion                        2.1.2-SNAPSHOT                                               #provide if different spark versions are involved
spark.restApi                                   http://chile-ubuntu:6066                           #provide! if not spark.master with port 6066 is assumed
spark.jars                                      file:///home/chile/IdeaProjects/unifiedviews-poc/spark-core/target/spark-core-1.0-SNAPSHOT-jar-with-dependencies.jar


### (template: 'spark.{spark.app.name}. '')
### ALIGNED use case specific properties ###
spark.uv-piepeline.ppx				            file:///home/chile/unifiedviews/outputs_of_PPX.nt
spark.uv-piepeline.typefilter		            https://w3id.org/dio#DesignRequirement,https://w3id.org/dio#DesignIntent,https://w3id.org/dio#MandatedSolution,https://w3id.org/dio#Argument,https://w3id.org/diopp#DeveloperIssue,https://w3id.org/diopp#SupportIssue,https://w3id.org/diopp#IdeaIssue,https://w3id.org/dio#Comment
spark.uv-piepeline.predicatefilter	            http://purl.org/dc/elements/1.1/title,http://purl.org/dc/elements/1.1/description
spark.uv-piepeline.filemanager.inputdir		    file:///home/chile/unifiedviews                     #provide!
spark.uv-piepeline.filemanager.inputfiles       inputsToSparkFragment.nt                            #provide!
spark.uv-piepeline.filemanager.outputdir		file:///home/chile/unifiedviews/alignedUseCase      #provide!
spark.uv-piepeline.filemanager.filePattern		annotations_found.ttl                               #provide!
spark.uv-piepeline.pipelineInitialize		    org.dbpedia.spark.pipelines.AlignedPipeline         #provide!
spark.uv-piepeline.executorArguments		    arg1,arg2                                           #arguments for main method in SparkPipelineExecutor (mainly for debugging!)

### DBpedia use case properties ###
spark.dbpedialinks.filemanager.inputdir		    file:///home/chile/unifiedviews                     #provide!
spark.dbpedialinks.filemanager.inputfiles       sameas_all_wikis.tql                                #provide!
spark.dbpedialinks.filemanager.outputdir		file:///home/chile/unifiedviews/dbpediaUseCase      #provide!
spark.dbpedialinks.filemanager.filePattern		interlangauge-links_$key.tql                        #provide!
spark.dbpedialinks.filemanager.compression		.gz                                                 #provide if you want the result to be compressed
spark.dbpedialinks.pairRddKeys		            de,fr                                                  #provide to split the result into multiple files, divided by these key (execute has to return a PairRdd, with these keys)
spark.dbpedialinks.pipelineInitialize		    org.dbpedia.spark.pipelines.DBpediaPipeline         #provide!
