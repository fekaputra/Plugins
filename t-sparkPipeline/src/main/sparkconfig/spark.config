# Default system properties included when running spark-submit.
# This is useful for setting default environmental settings.

spark.app.name						            uv-piepeline
#provide! -> the app.name defines the pipeline to run
spark.master                     	            spark://sparktest.semantic-web.at:7077
#spark://chile-ubuntu:7077
#provide! (you can use 'local[*]' to run on localhost)
#spark.eventLog.enabled           	            true
#spark.eventLog.dir               	            file:///home/chile/unifiedviews/events
# spark.serializer                 	            org.apache.spark.serializer.KryoSerializer
spark.driver.memory              	            8g
spark.executor.memory                           4g
# spark.executor.extraJavaOptions  	            -XX:+PrintGCDetails -Dkey=value -Dnumbers="one two three"
spark.shuffle.manager              	            SORT
spark.shuffle.consolidateFiles     	            true
spark.shuffle.spill                	            true
spark.shuffle.memoryFraction       	            0.75
spark.storage.memoryFraction       	            0.45
spark.shuffle.spill.compress       	            false
spark.shuffle.compress             	            false
#spark.dynamicAllocation.enabled                true

spark.client.version                            2.1.0
#provide if different spark versions are involved
spark.master.rest.port                          6066
#provide! if not spark.master with port 6066 is assumed
spark.jars                                      file:/home/sparktest/spark-core-1.0-SNAPSHOT-jar-with-dependencies.jar


### (template: 'spark.{spark.app.name}. '')
### DBpediaUseCase properties ###
spark.dbpedialinks.filemanager.inputdir		    file:///home/sparktest
#provide!
spark.dbpedialinks.filemanager.inputfiles       same_as_all_wikis.tql
#provide!
spark.dbpedialinks.filemanager.outputdir		file:///home/sparktest/dbpediaUseCase
#provide!
spark.dbpedialinks.filemanager.filePattern		interlangauge-links_$key.tql
#provide!
spark.dbpedialinks.filemanager.compression		gz
#provide if you want the result to be compressed
spark.dbpedialinks.pairRddKeys		            en,de,fr,pt,ar,vi,ja,el
#provide to split the result into multiple files, divided by these key (execute has to return a PairRdd, with these keys)
spark.dbpedialinks.pipeline.initializer		    org.dbpedia.spark.pipelines.AlignedPipeline
#provide!


### ALIGNED use case specific properties ###
## mandatory settings ##
spark.uv-piepeline.filemanager.inputdir		    file:///home/sparktest
#provide!
spark.uv-piepeline.filemanager.inputfiles       inputsToSparkFragment.nt
#provide!
spark.uv-piepeline.filemanager.outputdir		file:///home/sparktest/alignedUseCase
#provide!
spark.uv-piepeline.filemanager.filePattern		annotations_found.ttl
#provide!
spark.uv-piepeline.pipeline.initializer		    org.dbpedia.spark.pipelines.AlignedPipeline
#provide!

## additional use case specific properties ##
spark.uv-piepeline.typefilter		            https://w3id.org/dio#DesignRequirement,https://w3id.org/dio#DesignIntent,https://w3id.org/dio#MandatedSolution,https://w3id.org/dio#Argument,https://w3id.org/diopp#DeveloperIssue,https://w3id.org/diopp#SupportIssue,https://w3id.org/diopp#IdeaIssue,https://w3id.org/dio#Comment
spark.uv-piepeline.predicatefilter	            http://purl.org/dc/elements/1.1/title,http://purl.org/dc/elements/1.1/description

# PPX Local File settings
spark.uv-piepeline.ppx				            file:///home/sparktest/annotations_ppx.ttl

# PPX remote settings
spark.uv-piepeline.ppx.threadCount				8
spark.uv-piepeline.ppx.cacheSize				2000
# disable with 0
# spark.uv-piepeline.ppx.hostname				preview.poolparty.biz
# comment out to skip the ppx fetching (using precalculated file)
spark.uv-piepeline.ppx.port						80
spark.uv-piepeline.ppx.userName					knapt
spark.uv-piepeline.ppx.password					poolparty
spark.uv-piepeline.ppx.projectId				1DF14289-CE3D-0001-6075-47E36A00FAC0
spark.uv-piepeline.ppx.languageCode				en
spark.uv-piepeline.ppx.numberOfConcepts			10
spark.uv-piepeline.ppx.numberOfTerms			10
spark.uv-piepeline.ppx.extractionServiceUrl		/extractor/api/annotate
spark.uv-piepeline.ppx.extractionModelApi		/PoolParty/api/indexbuilder

