<html>
<head>
<link rel="stylesheet" type="text/css" href="style.css">
</head>
<body>
<h2 id="basicConcepts">Basic concepts for creating new Data Processing Units (DPUs) </h2>
        This section describes basic knowledge needed to prepare your own data processing units (DPUs) for ODCleanStore. 
        
        <h3>Types of DPUs</h3>
        <div>The application distinguished three types of DPUs: 
        <ul>
            <li>Extractor - DPU which extracts data from certain sources and produces outputs consumed by other DPUs. 
            	This DPU type has only output data units.</li>
            <li>Transformer - DPU which transform input data into output data. 
            	This DPU has input as well as output data units.</li>
            <li>Loader - DPU which save into DB/File/etc.. 
            	This DPU has only input data units. </li>
         </ul> 
         
         The DPU, you will be creating, must be one of the listed types.   
        </div>
		
          
          <h3 id="secConfigurationConcept">DPU Configuration</h3>
		<div>Every DPU has a possibility to store configuration object that parametrizes its 
			execution. DPU developer may provide the configuration dialog for his DPU, so that user may edit the configuration object via the 
			dialog - configuration object is synchronized with the dialog. 
			<br/>
			
			The configuration object must implement interface
			<div class="code"> <pre> <code>
interface cz.cuni.xrg.intlib.commons.configuration.DPUConfigObject {}  </code> </pre> </div> 
			which is located in commons. The interface requires implementation 
			of one method - isValid - which checks whether the configuration is valid, when the user input is stored from the configuration dialog to the configuration object.			</div>
                        
                        Configuration may be also used to prepare default configuration of the DPU - in that case, constructors without params is used to prepare configuration object and then this object is loaded to the confifuration dialog. 
                        
                        
        
          
            <h3>DPU Instance</h3>
          When DPU (e.g. RDF SPARQL Extractor, CSV Extractor, SILK Linker) is placed on the pipeline, such placement is called DPU instantiation and the result of such instantiation is called DPU instance. DPU instance has its own configuration being based on the template configuration of the DPU. DPU instance is always associated with the given pipeline. One pipeline may have more different DPU instances of the same DPU. 
          
                  <h3>DPU Template</h3>
          Every DPU is associated with a default (template) configuration (such configuration may contain, e.g., a file from which data should be by default extracted or the SILK linker rule, which should be by default applied). DPU together with its default (template) configuration is called DPU template. When the DPU is loaded to the system, a default configuration is added and used as the template configuration.
             
		
          <h3>Pipeline execution - Data Units</h3> <div>
		<p>Data between DPUs are passed in containers that are called 
		DataUnits. Therefore, data units can be comprehanded as data flow units, which describe data being passed between  DPUs.           
                Currently only RDF data units (units passing RDF data) are available. <i>RDFDataUnit</i> 
		provides methods for dealing with RDF content being passed from DPU to DPU. 
		The selected methods of RDFDataUnit interface, which can be used by the DPU developer for processing content of data units or for filling the data units with new RDF data, are: 
		<ul>
                        <li> Methods for adding new RDF triples to Data Unit
                        <ul>
                            <li>addTriple(String s, String p, String o) - Add the defined triple (s,p,o) to the DataUnit.  </li>
         <li>addTriplesFromFile(File file) - Add triples from file to the data unit. The file type is
	 determined by file's extension. </li>
                        <li>addTriplesFromFile(File file, RDFFormat format) - Add triples from file to the data unit. The file type is
	 explicitely specified </li>
         <li>addTriplesFromSPARQLEndpoint(URL endpointURL, String namedGraph) - Add triples from SPARQL endpoint specified by the endpoint URL and named graph to the data unit. </li>
           <li>addTriplesFromSPARQLEndpoint(URL endpointURL, String namedGraph,String user, String password) - Add triples from SPARQL endpoint specified by the endpoint URL, named graph to the data unit, using user and password provided. </li>

                        </ul>
                        </li>  
                        
                        <li> Methods for retrieving RDF triples from Data Unit
                        <ul>
                            <li>List<Statement> getTriples() - Get list of all RDF triples (Statements) in DataUnit.</li>
                            <li>saveTriplesToFile(File file, RDFFormatType formatType)  - Saves triples from data unit to file. The file type is
	 explicitely specified. </li>
                            <li>executeSelectQuery(String selectQuery) - Execute SPARQL select query over RDF data in DataUnit.</li>
                            <li>executeConstructQuery(String selectQuery) - Execute SPARQL construct query over RDF data in DataUnit.</li>
                            
                        </ul>
                        </li>  
                        
                        <li> Methods for transforming RDF data via queries
                        <ul>
                            <li>transform(String updateQuery) - Transform RDF data in DataUnit using SPARQL (update) query </li>
                            
                            
                        
                            
                        </ul>
                        </li>
                        
                           
                        
                            
                        </ul>
                        </li>
                        
                        
		</ul> For detailed information about the available methods please refer to the 
		<span class="inCode">RDFDataUnit</span> class </div> </p>
		  
		  
		    
		                
        <h3 id="context">Pipeline execution - DPU Context</h3> <div>
        <p>Each DPU has its execution context. The main interface is 
		<span class="inCode">ProcessingContext</span> located in commons module. 
		The DPU itself (based on its type -- extractor/transformer/loader) uses one of the more specialized types of context -  
		<span class="inCode">ExtractContext</span>, 
		<span class="inCode">TransformContext</span> or
		<span class="inCode">LoadContext</span>. </p> 
                
        <h4> Context provides access to the input/output data units. </h4> 
        Data units cannot be created directly. To create DataUnit use <span class="inCode">context</span> of the DPU. 
        
        <p>The following example demonstrate the functionality of adding new output data unit of the given DPU. By adding new data unit, you create new output data flow unit, which may be then filled with the data being outputted by that DPU. Every DPU may create one or more output data units.
<div class="code"> <pre> <code>
<span class="comment">// create output RDFDataUnit named "output_name"</span>
RDFDataUnit outputDataUnit = 
		(RDFDataUnit) context.addOutputDataUnit(DataUnitType.RDF, "output_name");
</code> </pre> </div> </p>			
		 
		 
		 
		<p>The following example demonstrate the functionality od obtaining first input data unit for DPU:
<div class="code"> <pre> <code>
<span class="comment">// get first input data unit </span><!--DataUnitList&lt;RDFDataUnit&gt; dataUnitList = RDFDataUnitList.create(context);-->
RDFDataUnit intputDataUnit = RDFDataUnitList.create(context).getFirst();
</code> </pre> </div>	</p>	 
		 
		 
		<p>
		In the following example we will pick the fist input data unit of type RDFDataUnit with the name "Astar":
<div class="code"> <pre> <code>
<span class="comment">// get first input data unit named "Astar"</span>
RDFDataUnit intputDataUnit = RDFDataUnitList.create(context).filterByName("Astar").getFirst();
</code> </pre> </div>	</p>
		
		<p>
		If the required data unit is not present, then the exception about
		MissingInput is automatically thrown and the execution is terminated.
		</div>
        
        
        <h4> Context provides access to the external environment of the DPU, such as to the working directories of DPUs. </h4>
         There are three places where DPU can store it's temporary files needed for its execution.
		<ul>
			<li><b>working directory</b> - This directory is unique for single DPU instance (DPU placement on the pipeline) and the particular run of hte pipeline. Only currently running DPUs can see such directory, which is private for the given DPU instance and pipeline run. Working directory may be obtained by calling <span class="code">context.getWorkingDir();</span> </li>
			<li><b>user directory</b> - This directory is unique for every DPU and the user who executes the DPU. Therefore, you can store DPU's user related data here; such data are visible to all instances od such DPU executed by that user. Proper concurency mechanisms needed, this directory is shared. User directory may be obtained by calling <span class="code">context.getUserDirectory();</span> </li>
			<li><b>global directory</b> - This directory is unique for a single DPU. It is
			shared by different DPU instances, pipeline runs, and users. You can store files that
			should be visible to your DPU at any time, such as certain caching mechanisms. Proper concurency mechanisms needed, this directory is shared. Global directory may be obtained by calling <span class="code">context.getGlobalDirectory();</span> </li>
		</ul> 
                
                <p>
		These directories can be obtained by calling specific context's methods. Alternatively, you can use <b>FileManager</b> helper class.
		Let's demonstrate it's functionality on few examples.</p>
		
		<p>
		The following example demonstrates how to get path to the file in 
		the working	directory with the use of File Manager: 
<div class="code"> <pre> <code>
<span class="comment">// creates file manager</span>
FileManager fileManager = new FileManager(context);
<span class="comment">// obtains file in working directory</span>
File filePath = fileManager.getWorking().file("myFile.txt");
</code> </pre> </div></p>

                <p>
		The following example demonstrates how to get the path to the file in 
		the global sub-directory. The directory is automatically created.
<div class="code"> <pre> <code>		 
<span class="comment">// creates file manager</span>
FileManager fileManager = new FileManager(context);
<span class="comment">// obtains file in sub-directory in global directory</span>
File filePath = fileManager.getGlobal().directory("myDir").file("myFile.txt");
</code> </pre> </div> </p>		 
		 
		</div>
        
        
        <h4>The context provides ability 
		to send messages (events) of the DPU as it is executed. Such messages are stored and accessible to the user via the execution monitor->record detail. 
		</h4>
                
                 <p>
		The following example demonstrates how the context may send various messages about the execution of the DPU.
<div class="code"> <pre> <code>		 
<span class="comment">// sends error message</span>
context.sendMessage(MessageType.ERROR, "MalformedURLException: "
					+ ex.getMessage());
</code> </pre> </div> </p>
		
		
		
		
		
		
    
             
             
             
             
                        
                        <h3>Pipeline execution - Logging the DPU's activity</h3>
		<div>Every DPU should log extensively. To log in your DPU, use slf4j. 
			The log for each DPU is stored and accessible to the user by the 
			graphical user interface of the application.  
			If you are not familiar with sl4j, the following example 
			shows the basics.<br/>
			<div class="code"> <pre> <code>
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class MyDPU  {
	private final log = LoggerFactory.getLogger(MyDPU.class);
...
			</code> </pre> </div>
			
			<div class="code"> 
			<pre> <code>log.debug("Some message .. ");</code> <br/> <code>log.info("Some message .. ");</code> <br/> <code>log.warn("Some message .. ");</code> <br/> <code>log.error("Some message .. ");</code></pre> </div>
		</div>
	
	As you can see, there are various levels for logging. Basic log levels are DEBUG, INFO, WARNING, ERROR. If you run pipeline, only WARNING+ logs are shown in the execution monitor. If you debug pipeline, all log levels are shown. 
                        
	
		
	
	
	
	
	
	

	<h2 id="basicTechs">Basic Technologies</h2>

	<h3 id="osgi">OSGi bundles</h3>
        <p>The DPUs developed are provided to the ODCleanStore in the form of OSGi bundles. The advantage of using OSGi bundles instead of simple JAR archives is that OSGi supports dynamic loadings of JAR files with their dependency resolving. Also every DPU may use its own set of dependencies (JAR libraries), which are hidden to other DPUs, thus, two DPU modules may use two conflicting dependencies at once. </p>  
        
        <p> Each OSGi bundle contains <b>maninifest.mf</b> file located in the bundle, META-INF folder. This files contains bundle description. The most important part contains settings for export-package and import-package. Sample of this part is as follows: 
	
	
<div class="maven-pom"> <pre> <code>
Export-Package: cz.cuni.mff.xrg.intlib.loader.rdf
Import-Package: com.vaadin.data,com.vaadin.data.util,com.vaadin.server,c
 om.vaadin.shared.ui.combobox,com.vaadin.ui,cz.cuni.xrg.intlib.commons.c
 onfiguration;version="0.0.1",cz.cuni.xrg.intlib.commons.data;version="0
 .0.1",cz.cuni.xrg.intlib.commons.loader;version="0.0.1",cz.cuni.xrg.int
 lib.commons.module.data;version="0.0.1",cz.cuni.xrg.intlib.commons.modu
 le.dialog;version="0.0.1",cz.cuni.xrg.intlib.commons.module.dpu;version
 ="0.0.1",cz.cuni.xrg.intlib.commons.web;version="0.0.1",cz.cuni.xrg.int
 lib.rdf.enums,cz.cuni.xrg.intlib.rdf.exceptions,cz.cuni.xrg.intlib.rdf.
 interfaces
</code> </pre> </div>

<p> The Export-Package configuration option specifies packages that the bundle provides to the external environment (ODCleanStore). On the other hand 
	Import-Package configuration option lists <b>dependencies</b> of the bundle -- the packages that are required by the bundle and MUST be provide by the external environment (ODCleanStore). 
	Before the bundle may be used by the ODCleanStore, all the dependencies of the bundle must be satisfied (resolved) - all packages listed in the Import-Package option must be provided by the ODCleanStore. Dependencies are specified as packages, not JAR archives or classes in the manisfest.mf file. </p>
        
        
        
        Manifest file <b>maninifest.mf</b> also contains lines with <b>DPU-MainClass</b> and
	<b>DPU-Package</b> that are essential for bundle use. DPU-MainClass points to the main class of the DPU package which is called when DPU is executed on the pipeline. DPU-Package contains name of the OSGi bundle (package).
        
        <h3>Maven</h3>
        <p><a href="http://maven.apache.org/">Apache Maven</a> is a software project management and comprehension tool. 
        The created DPUs are suggested to be maven projects, so that DPU bundles can be easily managed and created. In order to set up maven in your system, please follow <a href="http://maven.apache.org/run-maven/index.html">this site</a> </p>
        
        <p>TODO describe the minimum install - local repository? </p>
         <p>TODO describe the work with dependencies, scopes - compile, .. </p>
            

	<p>Pom.xml file is the basic configuration file needed when the maven project is being build. If you want to add dependency
	to the project pom.xml is the right place. Sample pom.xml file is
	attached in the sample DPU template bundle (see <a href="practise.html">DPU creation tutorial</a>).</p>
        

</body>
</html>